#!/usr/bin/env python3
"""
å¿«é€Ÿæ¼”ç¤ºè„šæœ¬ï¼šå±•ç¤ºçŸ¥è¯†åº“å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from orchestrator_agent import OrchestratorAgent

def main():
    print("ğŸš€ çŸ¥è¯†åº“å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    print("\n1ï¸âƒ£ åˆå§‹åŒ–ç³»ç»Ÿ...")
    orchestrator = OrchestratorAgent(storage_provider='memory')
    
    # æ·»åŠ ç¤ºä¾‹çŸ¥è¯†
    print("\n2ï¸âƒ£ æ·»åŠ ç¤ºä¾‹çŸ¥è¯†...")
    sample_knowledge = {
        "sources": [
            {
                "type": "text",
                "location": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿã€æ‰©å±•å’Œå¢å¼ºäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚AIç³»ç»Ÿå¯ä»¥æ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡ï¼Œå¦‚è§†è§‰è¯†åˆ«ã€è¯­éŸ³ç†è§£ã€å†³ç­–åˆ¶å®šå’Œè¯­è¨€ç¿»è¯‘ã€‚",
                "metadata": {"topic": "AI", "category": "technology"}
            },
            {
                "type": "text",
                "location": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå‘ç°æ¨¡å¼å’Œè§„å¾‹ï¼Œä»è€Œå¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚",
                "metadata": {"topic": "ML", "category": "technology"}
            },
            {
                "type": "text",
                "location": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPæŠ€æœ¯å¹¿æ³›åº”ç”¨äºèŠå¤©æœºå™¨äººã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œæƒ…æ„Ÿåˆ†æç­‰é¢†åŸŸã€‚",
                "metadata": {"topic": "NLP", "category": "technology"}
            }
        ]
    }
    
    result = orchestrator.receive_request("demo", "add_knowledge", sample_knowledge)
    if result.get("status") == "success":
        print(f"âœ… æˆåŠŸæ·»åŠ äº† {result.get('chunks_count')} ä¸ªçŸ¥è¯†å—")
    else:
        print(f"âŒ æ·»åŠ å¤±è´¥: {result.get('message')}")
        return
    
    # äº¤äº’å¼é—®ç­”
    print("\n3ï¸âƒ£ äº¤äº’å¼é—®ç­”æ¼”ç¤º")
    print("æ‚¨å¯ä»¥è¯¢é—®å…³äºAIã€æœºå™¨å­¦ä¹ ã€NLPçš„é—®é¢˜")
    print("è¾“å…¥ 'quit' é€€å‡º")
    
    while True:
        try:
            query = input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                break
            
            if not query:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
                continue
            
            print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢: {query}")
            result = orchestrator.receive_request("demo", "query", {"query": query})
            
            if result.get("status") == "success":
                print(f"\nğŸ’¡ ç­”æ¡ˆ:")
                print(result.get("answer", ""))
                print(f"\nğŸ“š æ‰¾åˆ° {result.get('sources_count', 0)} ä¸ªç›¸å…³çŸ¥è¯†æº")
            else:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result.get('message')}")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    print("\nğŸ‰ æ¼”ç¤ºç»“æŸ")

if __name__ == "__main__":
    main()