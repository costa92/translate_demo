#!/usr/bin/env python3
"""
é«˜çº§RAGæµ‹è¯• - æµ‹è¯•å¤æ‚æŸ¥è¯¢åœºæ™¯å’Œç³»ç»Ÿè¾¹ç•Œ
"""

import asyncio
import logging
import time
import json
from knowledge_base import KnowledgeBase, Config, Document
from knowledge_base.core.types import DocumentType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.ERROR)  # åªæ˜¾ç¤ºé”™è¯¯
logger = logging.getLogger(__name__)


# æ›´ä¸°å¯Œçš„æµ‹è¯•æ–‡æ¡£
ADVANCED_DOCUMENTS = [
    Document(
        id="python_advanced",
        content="""Pythoné«˜çº§ç‰¹æ€§åŒ…æ‹¬è£…é¥°å™¨ã€ç”Ÿæˆå™¨ã€ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œå…ƒç±»ã€‚è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°æˆ–ç±»çš„è¡Œä¸ºï¼Œç”Ÿæˆå™¨æä¾›å†…å­˜é«˜æ•ˆçš„è¿­ä»£æ–¹å¼ã€‚ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€šè¿‡__enter__å’Œ__exit__æ–¹æ³•ç®¡ç†èµ„æºï¼Œå…ƒç±»æ§åˆ¶ç±»çš„åˆ›å»ºè¿‡ç¨‹ã€‚Pythonè¿˜æ”¯æŒå¤šé‡ç»§æ‰¿ã€å±æ€§æè¿°ç¬¦ã€å¼‚æ­¥ç¼–ç¨‹ç­‰é«˜çº§æ¦‚å¿µã€‚GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰é™åˆ¶äº†Pythonçš„å¤šçº¿ç¨‹æ€§èƒ½ï¼Œä½†å¯ä»¥é€šè¿‡å¤šè¿›ç¨‹æˆ–å¼‚æ­¥ç¼–ç¨‹æ¥è§£å†³ã€‚""",
        type=DocumentType.TEXT,
        metadata={"topic": "python", "level": "advanced", "concepts": ["decorators", "generators", "metaclass"]}
    ),
    Document(
        id="ai_ethics",
        content="""äººå·¥æ™ºèƒ½ä¼¦ç†æ˜¯ç ”ç©¶AIç³»ç»Ÿå¯¹ç¤¾ä¼šã€ä¸ªäººå’Œç¯å¢ƒå½±å“çš„å­¦ç§‘ã€‚ä¸»è¦å…³æ³¨ç‚¹åŒ…æ‹¬ç®—æ³•åè§ã€éšç§ä¿æŠ¤ã€é€æ˜åº¦ã€è´£ä»»å½’å±å’Œç¤¾ä¼šå…¬å¹³ã€‚AIåè§å¯èƒ½æ¥æºäºè®­ç»ƒæ•°æ®ã€ç®—æ³•è®¾è®¡æˆ–åº”ç”¨åœºæ™¯ã€‚è§£å†³æ–¹æ¡ˆåŒ…æ‹¬å¤šæ ·åŒ–æ•°æ®é›†ã€å…¬å¹³æ€§çº¦æŸã€å¯è§£é‡ŠAIå’Œä¼¦ç†å®¡æŸ¥ã€‚AIæ²»ç†éœ€è¦æŠ€æœ¯ä¸“å®¶ã€æ”¿ç­–åˆ¶å®šè€…ã€ä¼¦ç†å­¦å®¶å’Œç¤¾ä¼šå„ç•Œçš„å…±åŒå‚ä¸ã€‚""",
        type=DocumentType.TEXT,
        metadata={"topic": "ai", "field": "ethics", "level": "advanced", "concerns": ["bias", "privacy", "fairness"]}
    ),
    Document(
        id="quantum_computing",
        content="""é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†ï¼Œå…·æœ‰è¶…è¶Šç»å…¸è®¡ç®—æœºçš„æ½œåŠ›ã€‚é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰å¯ä»¥åŒæ—¶å¤„äº0å’Œ1çš„å åŠ æ€ï¼Œé‡å­çº ç¼ å…è®¸é‡å­æ¯”ç‰¹ä¹‹é—´çš„ç¬æ—¶å…³è”ã€‚ä¸»è¦é‡å­ç®—æ³•åŒ…æ‹¬Shorç®—æ³•ï¼ˆå› æ•°åˆ†è§£ï¼‰å’ŒGroverç®—æ³•ï¼ˆæœç´¢ï¼‰ã€‚å½“å‰çš„é‡å­è®¡ç®—æœºä»å¤„äºNISQï¼ˆå™ªå£°ä¸­ç­‰è§„æ¨¡é‡å­ï¼‰é˜¶æ®µï¼Œé¢ä¸´é‡å­é€€ç›¸å¹²ã€é”™è¯¯ç‡é«˜ç­‰æŒ‘æˆ˜ã€‚IBMã€Googleã€å¾®è½¯ç­‰å…¬å¸åœ¨é‡å­è®¡ç®—é¢†åŸŸæŠ•å…¥å·¨å¤§ã€‚""",
        type=DocumentType.TEXT,
        metadata={"topic": "quantum", "field": "computing", "level": "expert", "algorithms": ["shor", "grover"]}
    ),
    Document(
        id="blockchain_tech",
        content="""åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦å“ˆå¸Œé“¾æ¥åŒºå—ï¼Œç¡®ä¿æ•°æ®ä¸å¯ç¯¡æ”¹ã€‚å…±è¯†æœºåˆ¶åŒ…æ‹¬å·¥ä½œé‡è¯æ˜ï¼ˆPoWï¼‰ã€æƒç›Šè¯æ˜ï¼ˆPoSï¼‰å’Œå§”æ‰˜æƒç›Šè¯æ˜ï¼ˆDPoSï¼‰ã€‚æ™ºèƒ½åˆçº¦æ˜¯è¿è¡Œåœ¨åŒºå—é“¾ä¸Šçš„è‡ªæ‰§è¡Œä»£ç ï¼Œä»¥å¤ªåŠæ˜¯æœ€è‘—åçš„æ™ºèƒ½åˆçº¦å¹³å°ã€‚åŒºå—é“¾åº”ç”¨åŒ…æ‹¬åŠ å¯†è´§å¸ã€ä¾›åº”é“¾ç®¡ç†ã€æ•°å­—èº«ä»½ã€å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰ç­‰ã€‚æŒ‘æˆ˜åŒ…æ‹¬å¯æ‰©å±•æ€§ã€èƒ½è€—ã€ç›‘ç®¡åˆè§„ç­‰é—®é¢˜ã€‚""",
        type=DocumentType.TEXT,
        metadata={"topic": "blockchain", "applications": ["cryptocurrency", "defi", "supply_chain"], "level": "intermediate"}
    ),
    Document(
        id="cybersecurity_trends",
        content="""ç½‘ç»œå®‰å…¨å¨èƒä¸æ–­æ¼”è¿›ï¼ŒåŒ…æ‹¬é«˜çº§æŒç»­å¨èƒï¼ˆAPTï¼‰ã€å‹’ç´¢è½¯ä»¶ã€é›¶æ—¥æ¼æ´å’Œç¤¾ä¼šå·¥ç¨‹æ”»å‡»ã€‚é˜²æŠ¤ç­–ç•¥é‡‡ç”¨çºµæ·±é˜²å¾¡ç†å¿µï¼ŒåŒ…æ‹¬ç½‘ç»œåˆ†æ®µã€é›¶ä¿¡ä»»æ¶æ„ã€å¨èƒæƒ…æŠ¥å’Œè¡Œä¸ºåˆ†æã€‚æ–°å…´æŠ€æœ¯å¦‚AIå’Œæœºå™¨å­¦ä¹ è¢«ç”¨äºå¨èƒæ£€æµ‹å’Œå“åº”è‡ªåŠ¨åŒ–ã€‚äº‘å®‰å…¨ã€IoTå®‰å…¨å’Œç§»åŠ¨å®‰å…¨æˆä¸ºæ–°çš„å…³æ³¨ç‚¹ã€‚ç½‘ç»œå®‰å…¨äººæ‰çŸ­ç¼ºæ˜¯å…¨çƒæ€§é—®é¢˜ï¼Œéœ€è¦åŠ å¼ºæ•™è‚²åŸ¹è®­å’Œå›½é™…åˆä½œã€‚""",
        type=DocumentType.TEXT,
        metadata={"topic": "security", "threats": ["apt", "ransomware", "zero_day"], "level": "expert"}
    )
]

# å¤æ‚æŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹
COMPLEX_QUERIES = [
    # å¤šæ¦‚å¿µå…³è”æŸ¥è¯¢
    {
        "query": "Pythonè£…é¥°å™¨å’Œç”Ÿæˆå™¨çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "type": "comparison",
        "expected_concepts": ["decorators", "generators"],
        "difficulty": "medium"
    },
    {
        "query": "How does quantum entanglement relate to quantum computing algorithms?",
        "type": "relationship",
        "expected_concepts": ["quantum", "algorithms"],
        "difficulty": "hard"
    },
    
    # è·¨é¢†åŸŸæŸ¥è¯¢
    {
        "query": "AIä¼¦ç†åœ¨åŒºå—é“¾åº”ç”¨ä¸­çš„è€ƒè™‘å› ç´ ",
        "type": "cross_domain",
        "expected_concepts": ["ai", "ethics", "blockchain"],
        "difficulty": "hard"
    },
    {
        "query": "é‡å­è®¡ç®—å¯¹ç½‘ç»œå®‰å…¨çš„å½±å“",
        "type": "cross_domain",
        "expected_concepts": ["quantum", "security"],
        "difficulty": "hard"
    },
    
    # æ·±åº¦æŠ€æœ¯æŸ¥è¯¢
    {
        "query": "Shorç®—æ³•å¦‚ä½•å¨èƒç°æœ‰çš„åŠ å¯†ç³»ç»Ÿï¼Ÿ",
        "type": "technical_deep",
        "expected_concepts": ["shor", "encryption"],
        "difficulty": "expert"
    },
    {
        "query": "é›¶ä¿¡ä»»æ¶æ„å¦‚ä½•é˜²å¾¡APTæ”»å‡»ï¼Ÿ",
        "type": "technical_deep",
        "expected_concepts": ["zero_trust", "apt"],
        "difficulty": "expert"
    },
    
    # è¶‹åŠ¿åˆ†ææŸ¥è¯¢
    {
        "query": "æœªæ¥5å¹´ç½‘ç»œå®‰å…¨çš„ä¸»è¦æŒ‘æˆ˜",
        "type": "trend_analysis",
        "expected_concepts": ["security", "future"],
        "difficulty": "medium"
    },
    {
        "query": "é‡å­è®¡ç®—å•†ä¸šåŒ–çš„æ—¶é—´è¡¨å’Œéšœç¢",
        "type": "trend_analysis",
        "expected_concepts": ["quantum", "commercial"],
        "difficulty": "hard"
    },
    
    # è§£å†³æ–¹æ¡ˆæŸ¥è¯¢
    {
        "query": "å¦‚ä½•è§£å†³AIç®—æ³•ä¸­çš„åè§é—®é¢˜ï¼Ÿ",
        "type": "solution",
        "expected_concepts": ["ai", "bias", "fairness"],
        "difficulty": "medium"
    },
    {
        "query": "åŒºå—é“¾å¯æ‰©å±•æ€§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ",
        "type": "solution",
        "expected_concepts": ["blockchain", "scalability"],
        "difficulty": "medium"
    },
    
    # å¯¹æ¯”åˆ†ææŸ¥è¯¢
    {
        "query": "PoW vs PoSå…±è¯†æœºåˆ¶çš„ä¼˜ç¼ºç‚¹",
        "type": "comparison",
        "expected_concepts": ["pow", "pos", "consensus"],
        "difficulty": "medium"
    },
    {
        "query": "ä¼ ç»Ÿè®¡ç®—ä¸é‡å­è®¡ç®—çš„æ ¹æœ¬å·®å¼‚",
        "type": "comparison",
        "expected_concepts": ["classical", "quantum", "computing"],
        "difficulty": "medium"
    },
    
    # åº”ç”¨åœºæ™¯æŸ¥è¯¢
    {
        "query": "æ™ºèƒ½åˆçº¦åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­çš„å…·ä½“åº”ç”¨",
        "type": "application",
        "expected_concepts": ["smart_contract", "supply_chain"],
        "difficulty": "medium"
    },
    {
        "query": "Pythonå…ƒç±»åœ¨å®é™…é¡¹ç›®ä¸­çš„ä½¿ç”¨åœºæ™¯",
        "type": "application",
        "expected_concepts": ["metaclass", "practical"],
        "difficulty": "hard"
    },
    
    # æŒ‘æˆ˜æ€§æŸ¥è¯¢
    {
        "query": "å¦‚ä½•åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶å®ç°AIç³»ç»Ÿçš„é€æ˜åº¦ï¼Ÿ",
        "type": "paradox",
        "expected_concepts": ["privacy", "transparency", "ai"],
        "difficulty": "expert"
    },
    {
        "query": "é‡å­è®¡ç®—æœºå¦‚ä½•åŒæ—¶åˆ©ç”¨å åŠ æ€å’Œçº ç¼ æ€ï¼Ÿ",
        "type": "technical_complex",
        "expected_concepts": ["superposition", "entanglement"],
        "difficulty": "expert"
    }
]

# è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹
EDGE_CASES = [
    {"query": "", "expected": "validation_error"},
    {"query": "a", "expected": "low_confidence"},
    {"query": "xyz123ä¸å­˜åœ¨çš„æ¦‚å¿µ", "expected": "no_match"},
    {"query": "Python " * 50, "expected": "handle_repetition"},
    {"query": "éå¸¸éå¸¸éå¸¸é•¿çš„æŸ¥è¯¢" * 20, "expected": "handle_long_query"},
    {"query": "ğŸ¤–ğŸ”¬ğŸ’»ğŸš€", "expected": "handle_emoji"},
    {"query": "SELECT * FROM users WHERE id=1; DROP TABLE users;", "expected": "handle_injection"},
]


async def setup_advanced_kb():
    """è®¾ç½®é«˜çº§çŸ¥è¯†åº“"""
    config = Config()
    config.storage.provider = "memory"
    config.embedding.provider = "sentence_transformers"
    config.generation.provider = "ollama"
    
    kb = KnowledgeBase(config)
    await kb.initialize()
    
    print("ğŸ“š æ·»åŠ é«˜çº§æµ‹è¯•æ–‡æ¡£...")
    result = await kb.add_documents(ADVANCED_DOCUMENTS)
    
    if result.success:
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(ADVANCED_DOCUMENTS)} ä¸ªé«˜çº§æ–‡æ¡£ï¼Œç”Ÿæˆ {result.chunks_created} ä¸ªchunks")
    else:
        print(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {result.error_message}")
        return None
    
    return kb


async def run_complex_queries(kb: KnowledgeBase):
    """è¿è¡Œå¤æ‚æŸ¥è¯¢æµ‹è¯•"""
    print(f"\nğŸ§  å¼€å§‹å¤æ‚æŸ¥è¯¢æµ‹è¯• ({len(COMPLEX_QUERIES)} ä¸ªæŸ¥è¯¢)...\n")
    
    results = []
    difficulty_stats = {"medium": 0, "hard": 0, "expert": 0}
    type_stats = {}
    
    for i, test_case in enumerate(COMPLEX_QUERIES, 1):
        query = test_case["query"]
        query_type = test_case["type"]
        difficulty = test_case["difficulty"]
        expected_concepts = test_case["expected_concepts"]
        
        print(f"[{i:2d}/{len(COMPLEX_QUERIES)}] {query_type.upper()} ({difficulty})")
        print(f"    æŸ¥è¯¢: {query}")
        
        start_time = time.time()
        try:
            result = await kb.query(query, top_k=5)
            query_time = time.time() - start_time
            
            # è¯„ä¼°ç»“æœè´¨é‡
            quality_score = evaluate_result_quality(result, expected_concepts)
            
            print(f"    ç­”æ¡ˆ: {result.answer[:80]}...")
            print(f"    è´¨é‡: {quality_score:.2f}, ç½®ä¿¡åº¦: {result.confidence:.2f}, ç”¨æ—¶: {query_time:.3f}s")
            
            results.append({
                "query": query,
                "type": query_type,
                "difficulty": difficulty,
                "quality_score": quality_score,
                "confidence": result.confidence,
                "time": query_time,
                "source_count": len(result.sources)
            })
            
            difficulty_stats[difficulty] += 1
            type_stats[query_type] = type_stats.get(query_type, 0) + 1
            
        except Exception as e:
            print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            results.append({
                "query": query,
                "type": query_type,
                "difficulty": difficulty,
                "error": str(e),
                "time": time.time() - start_time
            })
        
        print()
    
    return results, difficulty_stats, type_stats


async def run_edge_case_tests(kb: KnowledgeBase):
    """è¿è¡Œè¾¹ç•Œæƒ…å†µæµ‹è¯•"""
    print("ğŸ”¬ è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("-" * 40)
    
    edge_results = []
    
    for i, test_case in enumerate(EDGE_CASES, 1):
        query = test_case["query"]
        expected = test_case["expected"]
        
        print(f"[{i}] æµ‹è¯•: {expected}")
        print(f"    æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        try:
            result = await kb.query(query)
            status = "handled"
            confidence = result.confidence
            print(f"    ç»“æœ: æ­£å¸¸å¤„ç†, ç½®ä¿¡åº¦: {confidence:.2f}")
        except Exception as e:
            status = "error"
            confidence = 0.0
            print(f"    ç»“æœ: å¼‚å¸¸ - {type(e).__name__}")
        
        edge_results.append({
            "query": query,
            "expected": expected,
            "status": status,
            "confidence": confidence
        })
        print()
    
    return edge_results


def evaluate_result_quality(result, expected_concepts):
    """è¯„ä¼°ç»“æœè´¨é‡"""
    # ç®€å•çš„è´¨é‡è¯„ä¼°ç®—æ³•
    base_score = result.confidence
    
    # æ£€æŸ¥ç­”æ¡ˆé•¿åº¦ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿éƒ½ä¸å¥½ï¼‰
    answer_length = len(result.answer)
    if 50 <= answer_length <= 300:
        length_bonus = 0.1
    elif 30 <= answer_length <= 500:
        length_bonus = 0.05
    else:
        length_bonus = -0.1
    
    # æ£€æŸ¥æ¥æºæ•°é‡
    source_bonus = min(0.1, len(result.sources) * 0.02)
    
    # æ¦‚å¿µåŒ¹é…æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    concept_bonus = 0.0
    answer_lower = result.answer.lower()
    for concept in expected_concepts:
        if concept.lower() in answer_lower:
            concept_bonus += 0.05
    
    quality_score = base_score + length_bonus + source_bonus + concept_bonus
    return min(1.0, max(0.0, quality_score))


def analyze_advanced_results(results, difficulty_stats, type_stats, edge_results):
    """åˆ†æé«˜çº§æµ‹è¯•ç»“æœ"""
    print("=" * 60)
    print("ğŸ“Š é«˜çº§RAGæµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 60)
    
    successful_results = [r for r in results if "error" not in r]
    failed_results = [r for r in results if "error" in r]
    
    print(f"å¤æ‚æŸ¥è¯¢æµ‹è¯•:")
    print(f"  æ€»æŸ¥è¯¢æ•°: {len(results)}")
    print(f"  æˆåŠŸæŸ¥è¯¢: {len(successful_results)} ({len(successful_results)/len(results)*100:.1f}%)")
    print(f"  å¤±è´¥æŸ¥è¯¢: {len(failed_results)} ({len(failed_results)/len(results)*100:.1f}%)")
    
    if successful_results:
        avg_quality = sum(r["quality_score"] for r in successful_results) / len(successful_results)
        avg_confidence = sum(r["confidence"] for r in successful_results) / len(successful_results)
        avg_time = sum(r["time"] for r in successful_results) / len(successful_results)
        
        print(f"\næˆåŠŸæŸ¥è¯¢ç»Ÿè®¡:")
        print(f"  å¹³å‡è´¨é‡åˆ†: {avg_quality:.2f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
        print(f"  å¹³å‡ç”¨æ—¶: {avg_time:.3f}s")
    
    # éš¾åº¦åˆ†æ
    print(f"\néš¾åº¦åˆ†å¸ƒ:")
    for difficulty, count in difficulty_stats.items():
        success_rate = len([r for r in successful_results if r["difficulty"] == difficulty]) / count * 100
        print(f"  {difficulty}: {count}ä¸ªæŸ¥è¯¢, æˆåŠŸç‡: {success_rate:.1f}%")
    
    # ç±»å‹åˆ†æ
    print(f"\næŸ¥è¯¢ç±»å‹åˆ†å¸ƒ:")
    for query_type, count in type_stats.items():
        success_rate = len([r for r in successful_results if r["type"] == query_type]) / count * 100
        print(f"  {query_type}: {count}ä¸ªæŸ¥è¯¢, æˆåŠŸç‡: {success_rate:.1f}%")
    
    # è¾¹ç•Œæƒ…å†µåˆ†æ
    print(f"\nè¾¹ç•Œæƒ…å†µæµ‹è¯•:")
    handled_count = len([r for r in edge_results if r["status"] == "handled"])
    error_count = len([r for r in edge_results if r["status"] == "error"])
    print(f"  æ­£å¸¸å¤„ç†: {handled_count}/{len(edge_results)}")
    print(f"  å¼‚å¸¸æƒ…å†µ: {error_count}/{len(edge_results)}")
    
    # æ€§èƒ½åˆ†æ
    if successful_results:
        times = [r["time"] for r in successful_results]
        print(f"\næ€§èƒ½åˆ†æ:")
        print(f"  æœ€å¿«æŸ¥è¯¢: {min(times):.3f}s")
        print(f"  æœ€æ…¢æŸ¥è¯¢: {max(times):.3f}s")
        print(f"  æ—¶é—´ä¸­ä½æ•°: {sorted(times)[len(times)//2]:.3f}s")


async def test_concurrent_queries(kb: KnowledgeBase):
    """æµ‹è¯•å¹¶å‘æŸ¥è¯¢æ€§èƒ½"""
    print("\nâš¡ å¹¶å‘æŸ¥è¯¢æµ‹è¯•")
    print("-" * 40)
    
    # å‡†å¤‡å¹¶å‘æŸ¥è¯¢
    concurrent_queries = [
        "Pythonè£…é¥°å™¨çš„ä½œç”¨",
        "é‡å­è®¡ç®—çš„ä¼˜åŠ¿",
        "åŒºå—é“¾å…±è¯†æœºåˆ¶",
        "AIä¼¦ç†é—®é¢˜",
        "ç½‘ç»œå®‰å…¨å¨èƒ"
    ] * 4  # 20ä¸ªå¹¶å‘æŸ¥è¯¢
    
    print(f"æ‰§è¡Œ {len(concurrent_queries)} ä¸ªå¹¶å‘æŸ¥è¯¢...")
    
    start_time = time.time()
    
    # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
    tasks = [kb.query(query) for query in concurrent_queries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    total_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    successful = len([r for r in results if not isinstance(r, Exception)])
    failed = len([r for r in results if isinstance(r, Exception)])
    
    print(f"å¹¶å‘æµ‹è¯•ç»“æœ:")
    print(f"  æ€»æŸ¥è¯¢æ•°: {len(concurrent_queries)}")
    print(f"  æˆåŠŸæŸ¥è¯¢: {successful}")
    print(f"  å¤±è´¥æŸ¥è¯¢: {failed}")
    print(f"  æ€»ç”¨æ—¶: {total_time:.2f}s")
    print(f"  QPS: {len(concurrent_queries)/total_time:.1f}")
    print(f"  å¹³å‡å»¶è¿Ÿ: {total_time/len(concurrent_queries)*1000:.1f}ms")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ é«˜çº§RAGæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    try:
        # è®¾ç½®çŸ¥è¯†åº“
        kb = await setup_advanced_kb()
        if not kb:
            return
        
        # è¿è¡Œå¤æ‚æŸ¥è¯¢æµ‹è¯•
        results, difficulty_stats, type_stats = await run_complex_queries(kb)
        
        # è¿è¡Œè¾¹ç•Œæƒ…å†µæµ‹è¯•
        edge_results = await run_edge_case_tests(kb)
        
        # åˆ†æç»“æœ
        analyze_advanced_results(results, difficulty_stats, type_stats, edge_results)
        
        # å¹¶å‘æµ‹è¯•
        await test_concurrent_queries(kb)
        
        # å…³é—­çŸ¥è¯†åº“
        await kb.close()
        
        print("\nâœ… é«˜çº§RAGæµ‹è¯•å®Œæˆ!")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        test_report = {
            "timestamp": time.time(),
            "complex_queries": results,
            "edge_cases": edge_results,
            "difficulty_stats": difficulty_stats,
            "type_stats": type_stats
        }
        
        with open("advanced_rag_results.json", "w", encoding="utf-8") as f:
            json.dump(test_report, f, ensure_ascii=False, indent=2)
        
        print("ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° advanced_rag_results.json")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())